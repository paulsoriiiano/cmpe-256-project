{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Optimized ItemKNN Recommender with NDCG Evaluation\n",
                "\n",
                "This notebook implements an optimized ItemKNN model with proper NDCG evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from scipy.sparse import csr_matrix\n",
                "from sklearn.preprocessing import normalize\n",
                "import time"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(file_path):\n",
                "    print(\"Loading data...\")\n",
                "    rows, cols, data = [], [], []\n",
                "    user_history = {}\n",
                "    max_user_id, max_item_id = 0, 0\n",
                "    \n",
                "    with open(file_path, 'r') as f:\n",
                "        for line in f:\n",
                "            parts = list(map(int, line.strip().split()))\n",
                "            if not parts:\n",
                "                continue\n",
                "            user_id = parts[0]\n",
                "            items = parts[1:]\n",
                "            if not items:\n",
                "                continue\n",
                "                \n",
                "            user_history[user_id] = items\n",
                "            max_user_id = max(max_user_id, user_id)\n",
                "            for item_id in items:\n",
                "                rows.append(user_id)\n",
                "                cols.append(item_id)\n",
                "                data.append(1)\n",
                "                max_item_id = max(max_item_id, item_id)\n",
                "                \n",
                "    X = csr_matrix((data, (rows, cols)), shape=(max_user_id + 1, max_item_id + 1))\n",
                "    print(f\"Data loaded. Shape: {X.shape}, Non-zeros: {X.nnz}\")\n",
                "    return X, user_history\n",
                "\n",
                "train_file = '/Users/riteshsingh/Documents/SJSU/Recommender System/projectrec/train-2.txt'\n",
                "X_full, user_history_full = load_data(train_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train/Validation Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_train_val_split(user_history):\n",
                "    train_hist, val_hist = {}, {}\n",
                "    \n",
                "    for user, items in user_history.items():\n",
                "        if len(items) < 2:\n",
                "            train_hist[user] = items\n",
                "            val_hist[user] = []\n",
                "        else:\n",
                "            train_hist[user] = items[:-1]\n",
                "            val_hist[user] = [items[-1]]\n",
                "    \n",
                "    rows, cols, data = [], [], []\n",
                "    max_user, max_item = 0, 0\n",
                "    for user, items in train_hist.items():\n",
                "        max_user = max(max_user, user)\n",
                "        for item in items:\n",
                "            rows.append(user)\n",
                "            cols.append(item)\n",
                "            data.append(1)\n",
                "            max_item = max(max_item, item)\n",
                "    \n",
                "    X_train = csr_matrix((data, (rows, cols)), shape=(max_user + 1, max_item + 1))\n",
                "    return X_train, train_hist, val_hist\n",
                "\n",
                "X_train, train_hist, val_hist = create_train_val_split(user_history_full)\n",
                "print(f\"Train: {X_train.shape}, nnz: {X_train.nnz}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Compute Similarity Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_similarity(X, k=200):\n",
                "    print(\"Computing similarity matrix...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    X_norm = normalize(X, norm='l2', axis=0)\n",
                "    Sim = X_norm.T.dot(X_norm)\n",
                "    Sim.setdiag(0)\n",
                "    \n",
                "    print(f\"Similarity computed in {time.time() - start_time:.2f}s\")\n",
                "    return Sim\n",
                "\n",
                "Sim_train = compute_similarity(X_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. NDCG Calculation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dcg_at_k(relevances, k):\n",
                "    \"\"\"Calculate DCG@k\"\"\"\n",
                "    relevances = np.array(relevances)[:k]\n",
                "    if relevances.size:\n",
                "        return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
                "    return 0.0\n",
                "\n",
                "def ndcg_at_k(recommended, relevant, k=20):\n",
                "    \"\"\"Calculate NDCG@k\"\"\"\n",
                "    # Create relevance list for recommended items\n",
                "    rel_set = set(relevant)\n",
                "    relevances = [1 if item in rel_set else 0 for item in recommended[:k]]\n",
                "    \n",
                "    # DCG\n",
                "    dcg = dcg_at_k(relevances, k)\n",
                "    \n",
                "    # IDCG (ideal - all relevant items first)\n",
                "    ideal_relevances = [1] * min(len(rel_set), k) + [0] * (k - min(len(rel_set), k))\n",
                "    idcg = dcg_at_k(ideal_relevances, k)\n",
                "    \n",
                "    if idcg == 0:\n",
                "        return 0.0\n",
                "    return dcg / idcg\n",
                "\n",
                "def evaluate_ndcg(X, Sim, train_hist, val_hist, k=20):\n",
                "    print(f\"Evaluating NDCG@{k}...\")\n",
                "    \n",
                "    n_users = X.shape[0]\n",
                "    scores = X.dot(Sim)\n",
                "    \n",
                "    ndcg_scores = []\n",
                "    \n",
                "    for user in range(n_users):\n",
                "        if user not in val_hist or len(val_hist[user]) == 0:\n",
                "            continue\n",
                "        \n",
                "        user_scores = scores[user].toarray().flatten()\n",
                "        \n",
                "        # Mask training items\n",
                "        if user in train_hist:\n",
                "            user_scores[train_hist[user]] = -np.inf\n",
                "        \n",
                "        # Get top k\n",
                "        top_k_items = np.argsort(user_scores)[-k:][::-1]\n",
                "        \n",
                "        # Calculate NDCG\n",
                "        ndcg = ndcg_at_k(top_k_items, val_hist[user], k)\n",
                "        ndcg_scores.append(ndcg)\n",
                "    \n",
                "    avg_ndcg = np.mean(ndcg_scores)\n",
                "    print(f\"NDCG@{k}: {avg_ndcg:.4f} (evaluated {len(ndcg_scores)} users)\")\n",
                "    return avg_ndcg\n",
                "\n",
                "ndcg_itemknn = evaluate_ndcg(X_train, Sim_train, train_hist, val_hist, k=20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train on Full Data and Generate Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train on full data\n",
                "Sim_full = compute_similarity(X_full)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_recommendations(X, Sim, user_history, output_file, top_k=20):\n",
                "    print(\"Generating recommendations...\")\n",
                "    \n",
                "    n_users = X.shape[0]\n",
                "    batch_size = 1000\n",
                "    \n",
                "    with open(output_file, 'w') as f:\n",
                "        for start_idx in range(0, n_users, batch_size):\n",
                "            end_idx = min(start_idx + batch_size, n_users)\n",
                "            \n",
                "            user_batch = X[start_idx:end_idx]\n",
                "            scores_batch = user_batch.dot(Sim)\n",
                "            scores_dense = scores_batch.toarray()\n",
                "            \n",
                "            # Mask training items\n",
                "            mask = user_batch.toarray() > 0\n",
                "            scores_dense[mask] = -np.inf\n",
                "            \n",
                "            # Top k\n",
                "            top_items = np.argpartition(scores_dense, -top_k, axis=1)[:, -top_k:]\n",
                "            rows = np.arange(scores_dense.shape[0])[:, None]\n",
                "            top_scores = scores_dense[rows, top_items]\n",
                "            sort_ind = np.argsort(top_scores, axis=1)[:, ::-1]\n",
                "            final_recs = top_items[rows, sort_ind]\n",
                "            \n",
                "            for i, u_id in enumerate(range(start_idx, end_idx)):\n",
                "                recs = final_recs[i]\n",
                "                f.write(f\"{u_id} {' '.join(map(str, recs))}\\n\")\n",
                "                \n",
                "            if start_idx % 5000 == 0:\n",
                "                print(f\"Processed {start_idx} users...\")\n",
                "\n",
                "output_file = '/Users/riteshsingh/Documents/SJSU/Recommender System/projectrec/output.txt'\n",
                "generate_recommendations(X_full, Sim_full, user_history_full, output_file)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"ITEMKNN RECOMMENDER RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Validation NDCG@20: {ndcg_itemknn:.4f}\")\n",
                "print(f\"Recommendations saved to: {output_file}\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}