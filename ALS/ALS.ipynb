{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec8e6c6",
   "metadata": {},
   "source": [
    "# Alternating Least Squares\n",
    "Last Updated: Thursday, November 20, 7:08PM\n",
    "\n",
    "NDCG@20 (on 50%): 0.0372"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a07b0",
   "metadata": {},
   "source": [
    "## 1. Import and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1bb47df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (2380730, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2380730 entries, 0 to 2380729\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count    Dtype\n",
      "---  ------   --------------    -----\n",
      " 0   user_id  2380730 non-null  int64\n",
      " 1   item_id  2380730 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 36.3 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train_2_long.csv\")\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0315b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before dropping duplicates: 2380730\n",
      "Number of entries after dropping duplicates: 2380730\n"
     ]
    }
   ],
   "source": [
    "len_before = df.shape[0]\n",
    "print(f\"Number of entries before dropping duplicates: {len_before}\")\n",
    "df = df.drop_duplicates()\n",
    "len_after = df.shape[0]\n",
    "print(f\"Number of entries after dropping duplicates: {len_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "635def18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 52643 unique users.\n",
      "There are 91599 unique items.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df['user_id'].nunique()} unique users.\")\n",
    "print(f\"There are {df['item_id'].nunique()} unique items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "157eaace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average user interacted with 45 items.\n",
      "\n",
      "User interactions stats:\n",
      "count    52643.000000\n",
      "mean        45.224056\n",
      "std         77.958253\n",
      "min         16.000000\n",
      "25%         19.000000\n",
      "50%         26.000000\n",
      "75%         45.000000\n",
      "max      10682.000000\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "user_interactions = df[\"user_id\"].value_counts()\n",
    "print(f\"The average user interacted with {int(user_interactions.mean())} items.\\n\")\n",
    "print(f\"User interactions stats:\\n{user_interactions.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc513cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average item has 25 interactions.\n",
      "\n",
      "User interactions stats:\n",
      "count    91599.000000\n",
      "mean        25.990786\n",
      "std         38.397318\n",
      "min          1.000000\n",
      "25%         10.000000\n",
      "50%         15.000000\n",
      "75%         28.000000\n",
      "max       1741.000000\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "item_interactions = df[\"item_id\"].value_counts()\n",
    "print(f\"The average item has {int(item_interactions.mean())} interactions.\\n\")\n",
    "print(f\"User interactions stats:\\n{item_interactions.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b297c880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>150</td>\n",
       "      <td>9566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>150</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19001</th>\n",
       "      <td>150</td>\n",
       "      <td>5357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19002</th>\n",
       "      <td>150</td>\n",
       "      <td>13171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19003</th>\n",
       "      <td>150</td>\n",
       "      <td>13172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29676</th>\n",
       "      <td>150</td>\n",
       "      <td>21470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29677</th>\n",
       "      <td>150</td>\n",
       "      <td>21471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29678</th>\n",
       "      <td>150</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29679</th>\n",
       "      <td>150</td>\n",
       "      <td>21472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29680</th>\n",
       "      <td>150</td>\n",
       "      <td>7116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10682 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id\n",
       "18999      150     9566\n",
       "19000      150       60\n",
       "19001      150     5357\n",
       "19002      150    13171\n",
       "19003      150    13172\n",
       "...        ...      ...\n",
       "29676      150    21470\n",
       "29677      150    21471\n",
       "29678      150      552\n",
       "29679      150    21472\n",
       "29680      150     7116\n",
       "\n",
       "[10682 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_active_user = user_interactions.index[0]\n",
    "df[df[\"user_id\"] == most_active_user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebac74",
   "metadata": {},
   "source": [
    "## 2. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64516691",
   "metadata": {},
   "source": [
    "### 2.1. Encoding user and items\n",
    "`implicit` requires zero-based IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a26cce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID min: 0, max: 52642\n",
      "Item ID min: 6, max: 91604\n"
     ]
    }
   ],
   "source": [
    "# Check ID ranges\n",
    "print(f\"User ID min: {df['user_id'].min()}, max: {df['user_id'].max()}\")\n",
    "print(f\"Item ID min: {df['item_id'].min()}, max: {df['item_id'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bfbbdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map IDs to zero-based\n",
    "user_map = {uid: u_idx for u_idx, uid in enumerate(sorted(df[\"user_id\"].unique()))}\n",
    "item_map = {iid: i_idx for i_idx, iid in enumerate(sorted(df[\"item_id\"].unique()))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bfb2b",
   "metadata": {},
   "source": [
    "### 2.2. Function to convert to sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e5abd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def df_to_sparse(df):\n",
    "    \"\"\" Function that converts a dataframe to a sparse matrix. \"\"\"\n",
    "    rows = df[\"user_id\"].map(user_map)\n",
    "    cols = df[\"item_id\"].map(item_map)\n",
    "    vals = np.ones(len(df))\n",
    "\n",
    "    matrix = coo_matrix((vals, (rows, cols)),\n",
    "                        shape=(len(user_map), len(item_map)))\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6eb31",
   "metadata": {},
   "source": [
    "## 3. Split data into training and testing\n",
    "using `sklearn.model selection.train_test_split`\n",
    "\n",
    "* Split user-item interactions into training and testing.\n",
    "* Ensures a more realistic evaluation when generalizing to new users or items.\n",
    "\n",
    "For every user:\n",
    "* Perform `train_test_split` on the items it has interacted with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0cfe9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define group\n",
    "user_groups = df.groupby(\"user_id\")\n",
    "\n",
    "# Store splits\n",
    "train_pairs, test_pairs = [], []\n",
    "\n",
    "# Iterate through groups\n",
    "for user, items in user_groups:\n",
    "    train, test = train_test_split(items, test_size=0.2, random_state=17)\n",
    "    train_pairs.append(train)\n",
    "    test_pairs.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8f7acf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881725, 2) (499005, 2)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Turn lists into dataframes\n",
    "train_df = pd.concat(train_pairs)\n",
    "test_df = pd.concat(test_pairs)\n",
    "\n",
    "# Check dataframe shapes\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "# Make sure split is done properly\n",
    "print(train_df.shape[0] + test_df.shape[0] == df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3908810",
   "metadata": {},
   "source": [
    "## 4. Train ALS model\n",
    "Alternating Least Squares (ALS) is a matrix factorization algorithm which typically works well for implicit feedback data, such as user-item interactions, as seen in this [Netflix Prize and SVD](http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d17ca8",
   "metadata": {},
   "source": [
    "### 4.1. Create csr_matrix representations for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41f88a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 1881725 stored elements and shape (52643, 91599)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training matrix\n",
    "train_csr = df_to_sparse(train_df).tocsr()\n",
    "train_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c16a1ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 499005 stored elements and shape (52643, 91599)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test matrix\n",
    "test_csr = df_to_sparse(test_df).tocsr()\n",
    "test_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd01ec2",
   "metadata": {},
   "source": [
    "### 4.2. Train model\n",
    "Hyperparameters used for initial training:\n",
    "* `factors` = 50: the number of latent factors in for decomposition\n",
    "* `regularization` = 0.1: the regularization parameter ($\\lambda$)\n",
    "* `iterations` = 20: the number of training iterations\n",
    "* `alpha` = 1.0: the weights given for positive interactions\n",
    "* `use_cg` = True: used a faster solver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9cfcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11dd6dc72ca442f8773fecd2949d446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "model = AlternatingLeastSquares(\n",
    "    factors=50,             # hyperparameter (latent factors)\n",
    "    regularization=0.1,     # hyperparameter ()\n",
    "    iterations=20,          # hyperparameter (epochs)\n",
    "    alpha=1.0,              # hyperparameter (alpha)\n",
    "    use_cg=True,\n",
    "    calculate_training_loss=True,\n",
    "    random_state=17\n",
    ")\n",
    "\n",
    "model.fit(train_csr, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ad6af264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User factors shape: (52643, 50)\n",
      "Item factors shape: (91599, 50)\n"
     ]
    }
   ],
   "source": [
    "# Check matrix decomposition shapes\n",
    "print(\"User factors shape:\", model.user_factors.shape)\n",
    "print(\"Item factors shape:\", model.item_factors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8435d",
   "metadata": {},
   "source": [
    "## 5. Recommend top 20 items for users in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ef182",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd640533",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # top 20, NDCG@K\n",
    "\n",
    "recommended_items = model.recommend_all(train_csr, N=K, filter_already_liked_items=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69158eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that model recommended top 20 items for ALL users.\n",
    "len(recommended_items) == df[\"user_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85a966",
   "metadata": {},
   "source": [
    "## 6. Evaluate with NDCG@20\n",
    "For recommender systems:\n",
    "1. Create recommendations using `train_csr`\n",
    "2. Compare recommendations (predictions) from the `test_csr` (hold-out/validation set)\n",
    "3. Perform hyperparameter tuning on the model using the same steps above to find the best parameters for the model.\n",
    "4. Re-train the model with the entire dataset given the best hyperparameter values\n",
    "5. Generate recommendations from this fully-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7389846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NDCG@20 = 0.0460\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def batched_ndcg_at_k(recommended_items, test_matrix, k=20, batch_size=1000):\n",
    "    num_users = test_matrix.shape[0]\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for start in range(0, num_users, batch_size):\n",
    "        end = min(start + batch_size, num_users)\n",
    "\n",
    "        # Extract true relevance for the batch\n",
    "        true_batch = test_matrix[start:end].toarray()\n",
    "\n",
    "        # Build predicted score matrix for this batch\n",
    "        pred_batch = np.zeros_like(true_batch)\n",
    "        for i, user_idx in enumerate(range(start, end)):\n",
    "            items = recommended_items[user_idx]\n",
    "            pred_batch[i, items[:k]] = 1.0 / (np.arange(1, k + 1))  # rank weighting\n",
    "\n",
    "        # Compute vectorized NDCG for the batch\n",
    "        batch_ndcg = ndcg_score(true_batch, pred_batch, k=k)\n",
    "        ndcg_scores.append(batch_ndcg)\n",
    "\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Example usage\n",
    "mean_ndcg = batched_ndcg_at_k(recommended_items, test_csr, k=K, batch_size=1000)\n",
    "print(f\"Mean NDCG@20 = {mean_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ce849",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter tuning\n",
    "Tune hyperparameters mentioned in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fb789b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "647c6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(factors, reg, iters, alpha):\n",
    "    \"\"\" Evaluates model given certain hyperparameters. \"\"\"\n",
    "    model = AlternatingLeastSquares(\n",
    "        factors=factors,\n",
    "        regularization=reg,\n",
    "        iterations=iters,\n",
    "        alpha=alpha,\n",
    "        use_cg=True,\n",
    "        random_state=17         # ensure evaluation is consistent for each model state\n",
    "    )\n",
    "\n",
    "    model.fit(train_csr)\n",
    "\n",
    "    recs = model.recommend_all(train_csr, N=K, filter_already_liked_items=True)\n",
    "    ndcg = batched_ndcg_at_k(recs, test_csr, k=K, batch_size=1000)\n",
    "    \n",
    "    return (factors, reg, iters, alpha, ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d9b79602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to search\n",
    "param_grid = {\n",
    "    \"factors\": [64, 128, 256],\n",
    "    \"regularization\": [0.01, 0.05],\n",
    "    \"iterations\": [15, 30, 50, 100],\n",
    "    \"alpha\": [20, 40, 50]\n",
    "}\n",
    "\n",
    "\n",
    "param_combos = list(itertools.product(\n",
    "    param_grid[\"factors\"],\n",
    "    param_grid[\"regularization\"],\n",
    "    param_grid[\"iterations\"],\n",
    "    param_grid[\"alpha\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bc93f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "100%|██████████| 15/15 [00:15<00:00,  1.06s/it]]\n",
      "100%|██████████| 15/15 [00:15<00:00,  1.06s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.09s/it]]\n",
      "100%|██████████| 30/30 [00:32<00:00,  1.09s/it]]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.10s/it]]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.11s/it]\n",
      "100%|██████████| 50/50 [01:01<00:00,  1.23s/it]]\n",
      "100%|██████████| 50/50 [01:01<00:00,  1.23s/it]\n",
      "100%|██████████| 50/50 [01:01<00:00,  1.23s/it]]\n",
      "100%|██████████| 100/100 [02:13<00:00,  1.34s/it]\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]]\n",
      " 36%|███▌      | 36/100 [00:48<01:15,  1.18s/it][Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 24.6min\n",
      "100%|██████████| 15/15 [00:18<00:00,  1.22s/it]]\n",
      "100%|██████████| 15/15 [00:18<00:00,  1.21s/it]]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.17s/it]]\n",
      "100%|██████████| 30/30 [00:36<00:00,  1.21s/it]]\n",
      "100%|██████████| 30/30 [00:36<00:00,  1.20s/it]]\n",
      "100%|██████████| 50/50 [00:59<00:00,  1.20s/it]]\n",
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n",
      "100%|██████████| 100/100 [02:07<00:00,  1.27s/it]\n",
      "100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
      " 84%|████████▍ | 42/50 [01:03<00:11,  1.40s/it][Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 47.8min\n",
      "100%|██████████| 50/50 [01:15<00:00,  1.50s/it]\n",
      "100%|██████████| 15/15 [00:52<00:00,  3.49s/it]]\n",
      "100%|██████████| 15/15 [00:52<00:00,  3.52s/it]]\n",
      "100%|██████████| 15/15 [00:49<00:00,  3.31s/it]]\n",
      "100%|██████████| 100/100 [02:12<00:00,  1.33s/it]\n",
      "100%|██████████| 100/100 [02:14<00:00,  1.35s/it]\n",
      "100%|██████████| 30/30 [01:47<00:00,  3.58s/it]]\n",
      "100%|██████████| 100/100 [02:12<00:00,  1.32s/it]\n",
      "100%|██████████| 30/30 [01:46<00:00,  3.56s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.60s/it]\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 70.5min\n",
      "100%|██████████| 50/50 [03:10<00:00,  3.81s/it]\n",
      " 47%|████▋     | 7/15 [00:28<00:31,  3.95s/it]t][Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 75.3min\n",
      "100%|██████████| 15/15 [00:53<00:00,  3.57s/it]]\n",
      "100%|██████████| 15/15 [00:48<00:00,  3.25s/it]]\n",
      "100%|██████████| 15/15 [00:47<00:00,  3.17s/it]]\n",
      "100%|██████████| 50/50 [02:54<00:00,  3.49s/it]]\n",
      "100%|██████████| 50/50 [02:58<00:00,  3.58s/it]]\n",
      "100%|██████████| 30/30 [01:41<00:00,  3.39s/it]]\n",
      "100%|██████████| 100/100 [06:05<00:00,  3.65s/it]\n",
      "100%|██████████| 100/100 [06:06<00:00,  3.67s/it]\n",
      "100%|██████████| 100/100 [06:04<00:00,  3.64s/it]\n",
      "100%|██████████| 30/30 [01:51<00:00,  3.72s/it]\n",
      "100%|██████████| 30/30 [01:43<00:00,  3.45s/it]\n",
      "100%|██████████| 50/50 [02:43<00:00,  3.28s/it]]\n",
      "100%|██████████| 50/50 [02:43<00:00,  3.28s/it]]\n",
      "100%|██████████| 50/50 [02:44<00:00,  3.28s/it]]\n",
      "100%|██████████| 15/15 [02:21<00:00,  9.42s/it]]\n",
      "100%|██████████| 15/15 [02:22<00:00,  9.51s/it]]\n",
      "100%|██████████| 100/100 [05:39<00:00,  3.39s/it]\n",
      "100%|██████████| 100/100 [05:43<00:00,  3.43s/it]\n",
      "100%|██████████| 100/100 [06:05<00:00,  3.65s/it]\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed: 112.0min\n",
      "100%|██████████| 15/15 [02:31<00:00, 10.13s/it]\n",
      "100%|██████████| 30/30 [05:02<00:00, 10.07s/it]\n",
      "100%|██████████| 30/30 [04:53<00:00,  9.80s/it]\n",
      "100%|██████████| 30/30 [04:54<00:00,  9.82s/it]\n",
      "100%|██████████| 50/50 [08:16<00:00,  9.92s/it]]\n",
      "100%|██████████| 50/50 [08:17<00:00,  9.95s/it]]\n",
      "100%|██████████| 50/50 [08:17<00:00,  9.95s/it]]\n",
      "100%|██████████| 15/15 [02:32<00:00, 10.16s/it]]\n",
      " 86%|████████▌ | 86/100 [14:32<02:28, 10.57s/it][Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed: 138.2min\n",
      "100%|██████████| 100/100 [16:55<00:00, 10.16s/it]\n",
      "100%|██████████| 15/15 [02:33<00:00, 10.22s/it]]\n",
      "100%|██████████| 100/100 [16:56<00:00, 10.17s/it]\n",
      "100%|██████████| 100/100 [16:59<00:00, 10.19s/it]\n",
      "100%|██████████| 15/15 [02:35<00:00, 10.38s/it]\n",
      "100%|██████████| 30/30 [05:18<00:00, 10.62s/it]\n",
      "100%|██████████| 30/30 [05:17<00:00, 10.60s/it]\n",
      "100%|██████████| 30/30 [05:07<00:00, 10.25s/it]\n",
      "100%|██████████| 50/50 [08:38<00:00, 10.37s/it]\n",
      "  8%|▊         | 4/50 [00:41<07:53, 10.29s/it]][Parallel(n_jobs=-1)]: Done  61 out of  72 | elapsed: 163.1min remaining: 29.4min\n",
      "100%|██████████| 50/50 [08:38<00:00, 10.37s/it]]\n",
      "100%|██████████| 50/50 [08:06<00:00,  9.74s/it]]\n",
      "100%|██████████| 100/100 [14:49<00:00,  8.90s/it]\n",
      "100%|██████████| 100/100 [14:43<00:00,  8.83s/it]\n",
      "100%|██████████| 100/100 [14:34<00:00,  8.74s/it]\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  72 | elapsed: 181.4min remaining:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed: 188.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Parallel grid search for parameters\n",
    "results = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(evaluate_model)(f, r, i, a) for (f, r, i, a) in param_combos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e848b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display table of grid search\n",
    "pd.DataFrame(results, columns=[\"factors\", \"reg\", \"iterations\", \"alpha\", \"ndcg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4b3dcbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "factors=256, reg=0.05, iterations=50, alpha=40\n",
      "Best NDCG@20=0.1107\n"
     ]
    }
   ],
   "source": [
    "# Get best parameters\n",
    "best_params = max(results, key=lambda x: x[4])      #x[4] is ndcg\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(f\"factors={best_params[0]}, reg={best_params[1]}, iterations={best_params[2]}, alpha={best_params[3]}\")\n",
    "print(f\"Best NDCG@20={best_params[4]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4d5a4",
   "metadata": {},
   "source": [
    "## 9. Train model on full dataset with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17071ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters from the above cell\n",
    "FACTORS = 256\n",
    "REGULARIZATION = 0.05\n",
    "ITERATIONS = 50\n",
    "ALPHA = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9c73a2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e45ed9d19a4833809eceb159903499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create full dataset csr_matrix\n",
    "full_csr = df_to_sparse(df).tocsr()\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "model = AlternatingLeastSquares(\n",
    "    factors=FACTORS,\n",
    "    regularization=REGULARIZATION,\n",
    "    iterations=ITERATIONS,\n",
    "    alpha=ALPHA,\n",
    "    use_cg=True,\n",
    "    calculate_training_loss=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(full_csr, show_progress=True)\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = model.recommend_all(full_csr, N=K, filter_already_liked_items=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161cf19",
   "metadata": {},
   "source": [
    "## 10. Write output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4924da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse maps for user_idx and item_idx\n",
    "reverse_user_map = {user_idx: user_id for user_id, user_idx in user_map.items()}\n",
    "reverse_item_map = {item_idx: item_id for item_id, item_idx in item_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5454c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"als_tuned.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for user_idx, item_indices in enumerate(recommendations):\n",
    "        user_id = reverse_user_map[user_idx]\n",
    "        item_ids = [str(reverse_item_map[i]) for i in item_indices]\n",
    "        f.write(f\"{user_id} {' '.join(item_ids)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
