{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Music Playlist Continuation Recommender System \n",
                "## CMPE 256 Group Project\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.sparse import csr_matrix\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from gensim.models import Word2Vec\n",
                "from collections import Counter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_process_data(input_path, output_dir):\n",
                "   \n",
                "    print(f\"Loading data from {input_path}...\")\n",
                "    \n",
                "    \n",
                "    data = []\n",
                "    with open(input_path, 'r') as f:\n",
                "        for line in f:\n",
                "            parts = line.strip().split()\n",
                "            if not parts:\n",
                "                continue\n",
                "            user_id = parts[0]\n",
                "   \n",
                "            tracks = parts[1:]\n",
                "            \n",
                "            for track_id in tracks:\n",
                "                data.append([user_id, track_id])\n",
                "    \n",
                "    df = pd.DataFrame(data, columns=['user_id', 'item_id'])\n",
                "    \n",
                "    print(f\"Raw data shape: {df.shape}\")\n",
                "    \n",
                "    try:\n",
                "        df['user_id'] = df['user_id'].astype(int)\n",
                "        df['item_id'] = df['item_id'].astype(int)\n",
                "    except ValueError:\n",
                "        print(\"Warning: Could not convert IDs to integers. Keeping as strings.\")\n",
                "\n",
                "    original_count = len(df)\n",
                "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')\n",
                "    print(f\"Removed {original_count - len(df)} duplicate interactions.\")\n",
                "    \n",
                "    df = df.dropna()\n",
                "    \n",
                "    df = df[df['item_id'] != 0]\n",
                "    \n",
                "    print(f\"Cleaned data shape: {df.shape}\")\n",
                "    \n",
                "    if not os.path.exists(output_dir):\n",
                "        os.makedirs(output_dir)\n",
                "        \n",
                "    full_path = os.path.join(output_dir, 'interactions_full.csv')\n",
                "    df.to_csv(full_path, index=False)\n",
                "    print(f\"Saved full interactions to {full_path}\")\n",
                "    \n",
                "    print(\"Creating Train/Validation split...\")\n",
                "    \n",
                "    df['rank'] = df.groupby('user_id').cumcount() + 1\n",
                "    df['total_items'] = df.groupby('user_id')['item_id'].transform('count')\n",
                "    \n",
                "    df['split_point'] = (df['total_items'] * 0.8).astype(int)\n",
                "    \n",
                "    train_df = df[df['rank'] <= df['split_point']].copy()\n",
                "    val_df = df[df['rank'] > df['split_point']].copy()\n",
                "    \n",
                "    train_df = train_df.drop(columns=['rank', 'total_items', 'split_point'])\n",
                "    val_df = val_df.drop(columns=['rank', 'total_items', 'split_point'])\n",
                "    \n",
                "    print(f\"Train shape: {train_df.shape}\")\n",
                "    print(f\"Validation shape: {val_df.shape}\")\n",
                "    \n",
                "    train_path = os.path.join(output_dir, 'train_interactions.csv')\n",
                "    val_path = os.path.join(output_dir, 'val_interactions.csv')\n",
                "    \n",
                "    train_df.to_csv(train_path, index=False)\n",
                "    val_df.to_csv(val_path, index=False)\n",
                "    print(f\"Saved train split to {train_path}\")\n",
                "    print(f\"Saved validation split to {val_path}\")\n",
                "    \n",
                "    return train_df, val_df\n",
                "\n",
                "\n",
                "INPUT_FILE = \"train-2.txt\"\n",
                "OUTPUT_DIR = \"data/interim\"\n",
                "\n",
                "\n",
                "if os.path.exists(INPUT_FILE):\n",
                "    train_df, val_df = load_and_process_data(INPUT_FILE, OUTPUT_DIR)\n",
                "else:\n",
                "    print(f\"File {INPUT_FILE} not found. Assuming data is already processed in {OUTPUT_DIR}.\")\n",
                "    if os.path.exists(os.path.join(OUTPUT_DIR, 'train_interactions.csv')):\n",
                "        train_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_interactions.csv'))\n",
                "        val_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'val_interactions.csv'))\n",
                "        print(\"Loaded processed data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Recommender:\n",
                "    def fit(self, train_df):\n",
                "        raise NotImplementedError\n",
                "    \n",
                "    def recommend(self, user_id, n=20, already_seen=None):\n",
                "        raise NotImplementedError\n",
                "\n",
                "class PopularityRecommender(Recommender):\n",
                "    def __init__(self, exclude_top_percent=0.0):\n",
                "        self.popular_items = []\n",
                "        self.exclude_top_percent = exclude_top_percent\n",
                "        \n",
                "    def fit(self, train_df):\n",
                "        item_counts = train_df['item_id'].value_counts().reset_index()\n",
                "        item_counts.columns = ['item_id', 'count']\n",
                "        item_counts = item_counts.sort_values('count', ascending=False)\n",
                "        \n",
                "        if self.exclude_top_percent > 0:\n",
                "            num_exclude = int(len(item_counts) * self.exclude_top_percent)\n",
                "            item_counts = item_counts.iloc[num_exclude:]\n",
                "            \n",
                "        self.popular_items = item_counts['item_id'].tolist()\n",
                "        \n",
                "    def recommend(self, user_id, n=20, already_seen=None):\n",
                "        if already_seen is None:\n",
                "            already_seen = set()\n",
                "            \n",
                "        recs = []\n",
                "        for item in self.popular_items:\n",
                "            if item not in already_seen:\n",
                "                recs.append(item)\n",
                "                if len(recs) == n:\n",
                "                    break\n",
                "        return recs\n",
                "\n",
                "class ItemCFRecommender(Recommender):\n",
                "    def __init__(self, similarity_metric='cosine', k_neighbors=50):\n",
                "        self.similarity_metric = similarity_metric\n",
                "        self.k_neighbors = k_neighbors\n",
                "        self.item_sim_matrix = None\n",
                "        self.user_item_matrix = None\n",
                "        self.item_to_idx = {}\n",
                "        self.idx_to_item = {}\n",
                "        self.user_to_idx = {}\n",
                "        \n",
                "    def fit(self, train_df):\n",
                "        users = train_df['user_id'].unique()\n",
                "        items = train_df['item_id'].unique()\n",
                "        \n",
                "        self.user_to_idx = {u: i for i, u in enumerate(users)}\n",
                "        self.item_to_idx = {item: i for i, item in enumerate(items)}\n",
                "        self.idx_to_item = {i: item for item, i in self.item_to_idx.items()}\n",
                "        \n",
                "        rows = train_df['user_id'].map(self.user_to_idx)\n",
                "        cols = train_df['item_id'].map(self.item_to_idx)\n",
                "        data = np.ones(len(train_df))\n",
                "        \n",
                "        self.user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(users), len(items)))\n",
                "        item_user_matrix = self.user_item_matrix.T\n",
                "        \n",
                "        if self.similarity_metric == 'cosine':\n",
                "            self.item_sim_matrix = cosine_similarity(item_user_matrix, dense_output=True)\n",
                "        elif self.similarity_metric == 'jaccard':\n",
                "            from sklearn.metrics.pairwise import pairwise_distances\n",
                "            item_user_bool = item_user_matrix.astype(bool)\n",
                "            dist_matrix = pairwise_distances(item_user_bool, metric='jaccard', n_jobs=-1)\n",
                "            self.item_sim_matrix = 1 - dist_matrix\n",
                "            \n",
                "        np.fill_diagonal(self.item_sim_matrix, 0)\n",
                "        \n",
                "        if self.k_neighbors is not None and self.k_neighbors > 0:\n",
                "            n_items = self.item_sim_matrix.shape[0]\n",
                "            if self.k_neighbors < n_items:\n",
                "                for i in range(n_items):\n",
                "                    row = self.item_sim_matrix[i]\n",
                "                    if len(row) > self.k_neighbors:\n",
                "                        top_k_idx = np.argpartition(row, -self.k_neighbors)[-self.k_neighbors:]\n",
                "                        mask = np.ones(n_items, dtype=bool)\n",
                "                        mask[top_k_idx] = False\n",
                "                        row[mask] = 0\n",
                "                        self.item_sim_matrix[i] = row\n",
                "            self.item_sim_matrix = csr_matrix(self.item_sim_matrix)\n",
                "        \n",
                "    def recommend(self, user_id, n=20, already_seen=None):\n",
                "        if already_seen is None:\n",
                "            already_seen = set()\n",
                "        if user_id not in self.user_to_idx:\n",
                "            return []\n",
                "        u_idx = self.user_to_idx[user_id]\n",
                "        user_vector = self.user_item_matrix[u_idx].toarray().flatten()\n",
                "        scores = user_vector.dot(self.item_sim_matrix)\n",
                "        if hasattr(scores, 'toarray'):\n",
                "             scores = scores.toarray().flatten()\n",
                "        k = n + len(already_seen) + 50\n",
                "        top_indices = np.argsort(scores)[::-1][:k]\n",
                "        recs = []\n",
                "        for idx in top_indices:\n",
                "            item = self.idx_to_item[idx]\n",
                "            if item not in already_seen:\n",
                "                recs.append(item)\n",
                "                if len(recs) == n:\n",
                "                    break\n",
                "        return recs\n",
                "\n",
                "    def recommend_batch(self, user_ids, n=20, train_interactions=None):\n",
                "        u_indices = [self.user_to_idx[u] for u in user_ids if u in self.user_to_idx]\n",
                "        if not u_indices:\n",
                "            return {}\n",
                "        user_vectors = self.user_item_matrix[u_indices]\n",
                "        scores = user_vectors.dot(self.item_sim_matrix)\n",
                "        results = {}\n",
                "        valid_users = [u for u in user_ids if u in self.user_to_idx]\n",
                "        for i, user_id in enumerate(valid_users):\n",
                "            if hasattr(scores[i], 'toarray'):\n",
                "                user_scores = scores[i].toarray().flatten()\n",
                "            else:\n",
                "                user_scores = scores[i]\n",
                "            seen = train_interactions.get(user_id, set()) if train_interactions else set()\n",
                "            k_cand = n + len(seen) + 50\n",
                "            if k_cand > len(user_scores):\n",
                "                k_cand = len(user_scores)\n",
                "            top_indices_unsorted = np.argpartition(user_scores, -k_cand)[-k_cand:]\n",
                "            top_scores = user_scores[top_indices_unsorted]\n",
                "            sorted_indices_local = np.argsort(top_scores)[::-1]\n",
                "            top_indices = top_indices_unsorted[sorted_indices_local]\n",
                "            recs = []\n",
                "            for idx in top_indices:\n",
                "                item = self.idx_to_item[idx]\n",
                "                if item not in seen:\n",
                "                    recs.append(item)\n",
                "                    if len(recs) == n:\n",
                "                        break\n",
                "            results[user_id] = recs\n",
                "        return results\n",
                "\n",
                "class SVDRecommender(Recommender):\n",
                "    def __init__(self, n_components=50):\n",
                "        self.n_components = n_components\n",
                "        self.user_vecs = None\n",
                "        self.item_vecs = None\n",
                "        self.user_to_idx = {}\n",
                "        self.item_to_idx = {}\n",
                "        self.idx_to_item = {}\n",
                "        \n",
                "    def fit(self, train_df):\n",
                "        users = train_df['user_id'].unique()\n",
                "        items = train_df['item_id'].unique()\n",
                "        self.user_to_idx = {u: i for i, u in enumerate(users)}\n",
                "        self.item_to_idx = {item: i for i, item in enumerate(items)}\n",
                "        self.idx_to_item = {i: item for item, i in self.item_to_idx.items()}\n",
                "        rows = train_df['user_id'].map(self.user_to_idx)\n",
                "        cols = train_df['item_id'].map(self.item_to_idx)\n",
                "        data = np.ones(len(train_df))\n",
                "        user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(users), len(items)))\n",
                "        svd = TruncatedSVD(n_components=self.n_components, random_state=42)\n",
                "        self.user_vecs = svd.fit_transform(user_item_matrix)\n",
                "        self.item_vecs = svd.components_.T\n",
                "        \n",
                "    def recommend(self, user_id, n=20, already_seen=None):\n",
                "        if already_seen is None:\n",
                "            already_seen = set()\n",
                "        if user_id not in self.user_to_idx:\n",
                "            return []\n",
                "        u_idx = self.user_to_idx[user_id]\n",
                "        user_vec = self.user_vecs[u_idx]\n",
                "        scores = user_vec.dot(self.item_vecs.T)\n",
                "        k = n + len(already_seen) + 50\n",
                "        top_indices = np.argsort(scores)[::-1][:k]\n",
                "        recs = []\n",
                "        for idx in top_indices:\n",
                "            item = self.idx_to_item[idx]\n",
                "            if item not in already_seen:\n",
                "                recs.append(item)\n",
                "                if len(recs) == n:\n",
                "                    break\n",
                "        return recs\n",
                "\n",
                "    def recommend_batch(self, user_ids, n=20, train_interactions=None):\n",
                "        u_indices = [self.user_to_idx[u] for u in user_ids if u in self.user_to_idx]\n",
                "        if not u_indices:\n",
                "            return {}\n",
                "        user_vectors = self.user_vecs[u_indices]\n",
                "        scores = user_vectors.dot(self.item_vecs.T)\n",
                "        results = {}\n",
                "        valid_users = [u for u in user_ids if u in self.user_to_idx]\n",
                "        for i, user_id in enumerate(valid_users):\n",
                "            user_scores = scores[i]\n",
                "            seen = train_interactions.get(user_id, set()) if train_interactions else set()\n",
                "            k_cand = n + len(seen) + 50\n",
                "            if k_cand > len(user_scores):\n",
                "                k_cand = len(user_scores)\n",
                "            top_indices_unsorted = np.argpartition(user_scores, -k_cand)[-k_cand:]\n",
                "            top_scores = user_scores[top_indices_unsorted]\n",
                "            sorted_indices_local = np.argsort(top_scores)[::-1]\n",
                "            top_indices = top_indices_unsorted[sorted_indices_local]\n",
                "            recs = []\n",
                "            for idx in top_indices:\n",
                "                item = self.idx_to_item[idx]\n",
                "                if item not in seen:\n",
                "                    recs.append(item)\n",
                "                    if len(recs) == n:\n",
                "                        break\n",
                "            results[user_id] = recs\n",
                "        return results\n",
                "\n",
                "class Item2VecRecommender(Recommender):\n",
                "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
                "        self.vector_size = vector_size\n",
                "        self.window = window\n",
                "        self.min_count = min_count\n",
                "        self.model = None\n",
                "        \n",
                "    def fit(self, train_df):\n",
                "        items_str = train_df['item_id'].astype(str)\n",
                "        users = train_df['user_id']\n",
                "        temp_df = pd.DataFrame({'user': users, 'item': items_str})\n",
                "        sentences = temp_df.groupby('user')['item'].apply(list).tolist()\n",
                "        print(f\"Training Word2Vec on {len(sentences)} playlists...\")\n",
                "        self.model = Word2Vec(sentences=sentences, \n",
                "                              vector_size=self.vector_size, \n",
                "                              window=self.window, \n",
                "                              min_count=self.min_count, \n",
                "                              workers=4,\n",
                "                              sg=1,\n",
                "                              seed=42)\n",
                "        \n",
                "    def recommend(self, user_id, n=20, already_seen=None):\n",
                "        if already_seen is None:\n",
                "            already_seen = set()\n",
                "        user_history = [str(item) for item in already_seen if str(item) in self.model.wv]\n",
                "        if not user_history:\n",
                "            return []\n",
                "        recs_tuples = self.model.wv.most_similar(positive=user_history, topn=n + len(already_seen) + 20)\n",
                "        recs = []\n",
                "        for item_str, score in recs_tuples:\n",
                "            item = int(item_str)\n",
                "            if item not in already_seen:\n",
                "                recs.append(item)\n",
                "                if len(recs) == n:\n",
                "                    break\n",
                "        return recs\n",
                "\n",
                "class HybridRecommender(Recommender):\n",
                "    def __init__(self, models_with_weights):\n",
                "        self.models_with_weights = models_with_weights\n",
                "        \n",
                "    def fit(self, train_df):\n",
                "        for model, weight in self.models_with_weights:\n",
                "            print(f\"Fitting sub-model {type(model).__name__}...\")\n",
                "            model.fit(train_df)\n",
                "            \n",
                "    def recommend(self, user_id, n=20, already_seen=None):\n",
                "        k_cand = n * 5 \n",
                "        item_scores = {}\n",
                "        for model, weight in self.models_with_weights:\n",
                "            recs = model.recommend(user_id, n=k_cand, already_seen=already_seen)\n",
                "            for rank, item in enumerate(recs):\n",
                "                score = weight * (1.0 / (rank + 1))\n",
                "                item_scores[item] = item_scores.get(item, 0.0) + score\n",
                "        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
                "        final_recs = [item for item, score in sorted_items[:n]]\n",
                "        return final_recs\n",
                "\n",
                "    def recommend_batch(self, user_ids, n=20, train_interactions=None):\n",
                "        k_cand = n * 5\n",
                "        all_model_recs = []\n",
                "        for model, weight in self.models_with_weights:\n",
                "            if hasattr(model, 'recommend_batch'):\n",
                "                print(f\"Batch predicting with {type(model).__name__}...\")\n",
                "                recs = model.recommend_batch(user_ids, n=k_cand, train_interactions=train_interactions)\n",
                "                all_model_recs.append((recs, weight))\n",
                "            else:\n",
                "                print(f\"Sequential predicting with {type(model).__name__}...\")\n",
                "                recs = {}\n",
                "                for uid in user_ids:\n",
                "                    seen = train_interactions.get(uid, set()) if train_interactions else set()\n",
                "                    recs[uid] = model.recommend(uid, n=k_cand, already_seen=seen)\n",
                "                all_model_recs.append((recs, weight))\n",
                "        results = {}\n",
                "        for user_id in user_ids:\n",
                "            item_scores = {}\n",
                "            for model_recs, weight in all_model_recs:\n",
                "                recs = model_recs.get(user_id, [])\n",
                "                for rank, item in enumerate(recs):\n",
                "                    score = weight * (1.0 / (rank + 1))\n",
                "                    item_scores[item] = item_scores.get(item, 0.0) + score\n",
                "            sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
                "            results[user_id] = [item for item, score in sorted_items[:n]]\n",
                "        return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def precision_at_k(recommended, actual, k=20):\n",
                "    if not actual:\n",
                "        return 0.0\n",
                "    recommended = recommended[:k]\n",
                "    relevant = set(recommended) & set(actual)\n",
                "    return len(relevant) / k\n",
                "\n",
                "def recall_at_k(recommended, actual, k=20):\n",
                "    if not actual:\n",
                "        return 0.0\n",
                "    recommended = recommended[:k]\n",
                "    relevant = set(recommended) & set(actual)\n",
                "    return len(relevant) / len(actual)\n",
                "\n",
                "def ndcg_at_k(recommended, actual, k=20):\n",
                "    if not actual:\n",
                "        return 0.0\n",
                "    recommended = recommended[:k]\n",
                "    dcg = 0.0\n",
                "    idcg = 0.0\n",
                "    actual_set = set(actual)\n",
                "    for i, item in enumerate(recommended):\n",
                "        if item in actual_set:\n",
                "            dcg += 1.0 / np.log2(i + 2)\n",
                "    for i in range(min(len(actual), k)):\n",
                "        idcg += 1.0 / np.log2(i + 2)\n",
                "    return dcg / idcg if idcg > 0 else 0.0\n",
                "\n",
                "def evaluate_model(model, train_df, val_df, model_name):\n",
                "    print(f\"Training {model_name}...\")\n",
                "    start_time = time.time()\n",
                "    model.fit(train_df)\n",
                "    train_time = time.time() - start_time\n",
                "    print(f\"Training took {train_time:.2f}s\")\n",
                "    \n",
                "    print(f\"Evaluating {model_name}...\")\n",
                "    val_grouped = val_df.groupby('user_id')['item_id'].apply(list).to_dict()\n",
                "    train_grouped = train_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
                "    \n",
                "    precisions = []\n",
                "    recalls = []\n",
                "    ndcgs = []\n",
                "    \n",
                "    users_to_eval = list(val_grouped.keys())\n",
                "    if len(users_to_eval) > 10000:\n",
                "        print(f\"Limiting evaluation to 10,000 users (out of {len(users_to_eval)}) for speed.\")\n",
                "        users_to_eval = users_to_eval[:10000]\n",
                "    \n",
                "    start_eval = time.time()\n",
                "    if hasattr(model, 'recommend_batch'):\n",
                "        print(\"Using batch recommendation...\")\n",
                "        batch_size = 1000\n",
                "        for i in range(0, len(users_to_eval), batch_size):\n",
                "            batch_users = users_to_eval[i:i+batch_size]\n",
                "            batch_recs = model.recommend_batch(batch_users, n=20, train_interactions=train_grouped)\n",
                "            for user_id in batch_users:\n",
                "                recommendations = batch_recs.get(user_id, [])\n",
                "                actual_items = val_grouped[user_id]\n",
                "                precisions.append(precision_at_k(recommendations, actual_items, k=20))\n",
                "                recalls.append(recall_at_k(recommendations, actual_items, k=20))\n",
                "                ndcgs.append(ndcg_at_k(recommendations, actual_items, k=20))\n",
                "            if i % 5000 == 0:\n",
                "                print(f\"Evaluated {i}/{len(users_to_eval)} users...\")\n",
                "    else:\n",
                "        for i, user_id in enumerate(users_to_eval):\n",
                "            if i % 1000 == 0:\n",
                "                print(f\"Evaluated {i}/{len(users_to_eval)} users...\")\n",
                "            actual_items = val_grouped[user_id]\n",
                "            seen_items = train_grouped.get(user_id, set())\n",
                "            recommendations = model.recommend(user_id, n=20, already_seen=seen_items)\n",
                "            precisions.append(precision_at_k(recommendations, actual_items, k=20))\n",
                "            recalls.append(recall_at_k(recommendations, actual_items, k=20))\n",
                "            ndcgs.append(ndcg_at_k(recommendations, actual_items, k=20))\n",
                "            \n",
                "    eval_time = time.time() - start_eval\n",
                "    metrics = {\n",
                "        'Model': model_name,\n",
                "        'Precision@20': np.mean(precisions),\n",
                "        'Recall@20': np.mean(recalls),\n",
                "        'NDCG@20': np.mean(ndcgs),\n",
                "        'Train Time': train_time,\n",
                "        'Eval Time': eval_time\n",
                "    }\n",
                "    print(f\"Results for {model_name}:\")\n",
                "    print(metrics)\n",
                "    return metrics\n",
                "\n",
                "\n",
                "results = []\n",
                "\n",
                "results.append(evaluate_model(ItemCFRecommender(similarity_metric='cosine', k_neighbors=5), train_df, val_df, \"ItemCF (Cosine, k=5)\"))\n",
                "results.append(evaluate_model(ItemCFRecommender(similarity_metric='cosine', k_neighbors=10), train_df, val_df, \"ItemCF (Cosine, k=10)\"))\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nFinal Evaluation Results:\")\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "bars = plt.barh(results_df['Model'], results_df['NDCG@20'], color='skyblue')\n",
                "plt.xlabel('NDCG@20')\n",
                "plt.title('Model Comparison - NDCG@20')\n",
                "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
                "for bar in bars:\n",
                "    width = bar.get_width()\n",
                "    plt.text(width, bar.get_y() + bar.get_height()/2, f'{width:.4f}', ha='left', va='center', fontsize=10)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "x = range(len(results_df))\n",
                "width = 0.35\n",
                "plt.bar([i - width/2 for i in x], results_df['Precision@20'], width, label='Precision@20', color='lightgreen')\n",
                "plt.bar([i + width/2 for i in x], results_df['Recall@20'], width, label='Recall@20', color='salmon')\n",
                "plt.xlabel('Model')\n",
                "plt.ylabel('Score')\n",
                "plt.title('Model Comparison - Precision & Recall')\n",
                "plt.xticks(x, results_df['Model'], rotation=45, ha='right')\n",
                "plt.legend()\n",
                "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Submission Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_submission(model_name, output_file='submission_improved.txt'):\n",
                "    print(f\"Generating recommendations using {model_name}...\")\n",
                "    full_df = pd.read_csv('data/interim/interactions_full.csv')\n",
                "    \n",
                "    if 'Popularity' in model_name:\n",
                "        model = PopularityRecommender(exclude_top_percent=0.01)\n",
                "    elif 'ItemCF' in model_name:\n",
                "\n",
                "        model = ItemCFRecommender(similarity_metric='cosine', k_neighbors=5)\n",
                "    elif 'SVD' in model_name:\n",
                "        model = SVDRecommender(n_components=100)\n",
                "    elif 'Item2Vec' in model_name:\n",
                "        model = Item2VecRecommender(vector_size=100, window=10)\n",
                "    else:\n",
                "        model = ItemCFRecommender(similarity_metric='cosine', k_neighbors=5)\n",
                "        \n",
                "    print(\"Training on full dataset...\")\n",
                "    model.fit(full_df)\n",
                "    \n",
                "    users = full_df['user_id'].unique()\n",
                "    user_history = full_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
                "    \n",
                "    print(f\"Generating for {len(users)} users...\")\n",
                "    results = []\n",
                "    \n",
                "    if hasattr(model, 'recommend_batch'):\n",
                "        batch_size = 1000\n",
                "        users_list = list(users)\n",
                "        for i in range(0, len(users_list), batch_size):\n",
                "            batch_users = users_list[i:i+batch_size]\n",
                "            batch_recs = model.recommend_batch(batch_users, n=20, train_interactions=user_history)\n",
                "            for user_id, recs in batch_recs.items():\n",
                "                row = [user_id] + recs\n",
                "                results.append(row)\n",
                "            if i % 10000 == 0:\n",
                "                print(f\"Processed {i}/{len(users)} users...\")\n",
                "    else:\n",
                "        for i, user_id in enumerate(users):\n",
                "            seen = user_history.get(user_id, set())\n",
                "            recs = model.recommend(user_id, n=20, already_seen=seen)\n",
                "            row = [user_id] + recs\n",
                "            results.append(row)\n",
                "            if i % 5000 == 0:\n",
                "                print(f\"Processed {i}/{len(users)} users...\")\n",
                "                \n",
                "    cols = ['user_id'] + [f'item_{i+1}' for i in range(20)]\n",
                "    max_len = 21\n",
                "    cleaned_results = []\n",
                "    for row in results:\n",
                "        if len(row) < max_len:\n",
                "            row = row + [None] * (max_len - len(row))\n",
                "        cleaned_results.append(row)\n",
                "        \n",
                "    res_df = pd.DataFrame(cleaned_results, columns=cols)\n",
                "    res_df.to_csv(output_file, index=False, header=False)\n",
                "    print(f\"Saved recommendations to {output_file}\")\n",
                "\n",
                "\n",
                "best_model = results_df.loc[results_df['NDCG@20'].idxmax()]['Model']\n",
                "print(f\"Best model found: {best_model}\")\n",
                "generate_submission(best_model, output_file='output/submission_improved.txt')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
