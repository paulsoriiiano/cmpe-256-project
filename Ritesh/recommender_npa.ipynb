{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neural Recommendation with Personalized Attention (NPA)\n",
                "\n",
                "This notebook implements an Attention-based Neural Recommender System. \n",
                "Since the dataset consists only of user-item interactions (no text content), we adapt the NPA concept to use **User Attention over Interaction History**.\n",
                "\n",
                "## Architecture\n",
                "1.  **User Embedding**: Represents the user's intrinsic preference.\n",
                "2.  **Item Embedding**: Represents items.\n",
                "3.  **Attention Mechanism**: The User Embedding attends to the Item Embeddings in the user's history to create a dynamic user representation.\n",
                "4.  **Prediction**: Dot product between the dynamic user representation and the target item embedding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import time\n",
                "import random\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data...\n",
                        "Loaded 52643 users. Max User ID: 52642, Max Item ID: 91604\n"
                    ]
                }
            ],
            "source": [
                "def load_data(file_path):\n",
                "    print(\"Loading data...\")\n",
                "    user_history = {}\n",
                "    max_user_id = 0\n",
                "    max_item_id = 0\n",
                "    \n",
                "    with open(file_path, 'r') as f:\n",
                "        for line in f:\n",
                "            parts = list(map(int, line.strip().split()))\n",
                "            if not parts:\n",
                "                continue\n",
                "            user_id = parts[0]\n",
                "            items = parts[1:]\n",
                "            \n",
                "            if not items:\n",
                "                continue\n",
                "                \n",
                "            user_history[user_id] = items\n",
                "            max_user_id = max(max_user_id, user_id)\n",
                "            max_item_id = max(max_item_id, max(items))\n",
                "            \n",
                "    print(f\"Loaded {len(user_history)} users. Max User ID: {max_user_id}, Max Item ID: {max_item_id}\")\n",
                "    return user_history, max_user_id, max_item_id\n",
                "\n",
                "train_file = '/Users/riteshsingh/Documents/SJSU/Recommender System/projectrec/train-2.txt'\n",
                "user_history, n_users, n_items = load_data(train_file)\n",
                "\n",
                "# Adjust counts for 0-indexing if needed (IDs seem to be 0-indexed based on previous files)\n",
                "n_users += 1\n",
                "n_items += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RecommenderDataset(Dataset):\n",
                "    def __init__(self, user_history, n_items, history_len=20, num_negatives=4):\n",
                "        self.user_history = user_history\n",
                "        self.n_items = n_items\n",
                "        self.history_len = history_len\n",
                "        self.num_negatives = num_negatives\n",
                "        self.users = list(user_history.keys())\n",
                "        \n",
                "        self.samples = []\n",
                "        self._generate_samples()\n",
                "        \n",
                "    def _generate_samples(self):\n",
                "        print(\"Generating training samples...\")\n",
                "        for user in tqdm(self.users):\n",
                "            items = self.user_history[user]\n",
                "            if len(items) < 2:\n",
                "                continue\n",
                "                \n",
                "            # Leave one out for training target (or use sliding window)\n",
                "            # Here we use the last item as target, and previous as history\n",
                "            # To get more data, we can use sliding window\n",
                "            \n",
                "            # Simple approach: Use each item in history as a target, with previous items as history\n",
                "            # Limit to last few items to avoid exploding dataset size\n",
                "            \n",
                "            # Let's use the last item as target for validation-like structure,\n",
                "            # but for training we want to learn from all interactions.\n",
                "            # Given dataset size, let's sample 5 targets per user if available.\n",
                "            \n",
                "            targets = items[-5:] if len(items) > 5 else items[1:]\n",
                "            \n",
                "            for i, target in enumerate(targets):\n",
                "                # History is everything before this target\n",
                "                # We take the last 'history_len' items\n",
                "                target_idx = items.index(target)\n",
                "                history = items[:target_idx]\n",
                "                \n",
                "                if not history:\n",
                "                    continue\n",
                "                    \n",
                "                # Pad or truncate history\n",
                "                if len(history) > self.history_len:\n",
                "                    hist_seq = history[-self.history_len:]\n",
                "                else:\n",
                "                    hist_seq = history + [0] * (self.history_len - len(history))\n",
                "                \n",
                "                # Positive sample\n",
                "                self.samples.append((user, hist_seq, target, 1.0))\n",
                "                \n",
                "                # Negative samples\n",
                "                for _ in range(self.num_negatives):\n",
                "                    neg = random.randint(0, self.n_items - 1)\n",
                "                    while neg in items:\n",
                "                        neg = random.randint(0, self.n_items - 1)\n",
                "                    self.samples.append((user, hist_seq, neg, 0.0))\n",
                "                    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        user, history, item, label = self.samples[idx]\n",
                "        return torch.tensor(user), torch.tensor(history), torch.tensor(item), torch.tensor(label, dtype=torch.float32)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating training samples...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 52643/52643 [00:01<00:00, 48952.16it/s]\n"
                    ]
                }
            ],
            "source": [
                "# Hyperparameters\n",
                "EMBEDDING_DIM = 32\n",
                "HISTORY_LEN = 20\n",
                "BATCH_SIZE = 512\n",
                "EPOCHS = 3\n",
                "LR = 0.001\n",
                "\n",
                "dataset = RecommenderDataset(user_history, n_items, history_len=HISTORY_LEN)\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Definition (NPA-style)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NPARecommender(nn.Module):\n",
                "    def __init__(self, n_users, n_items, embedding_dim, history_len):\n",
                "        super(NPARecommender, self).__init__()\n",
                "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
                "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
                "        \n",
                "        # Attention Mechanism\n",
                "        # Query: User Embedding\n",
                "        # Key/Value: Item Embedding\n",
                "        self.attention_fc = nn.Linear(embedding_dim, embedding_dim)\n",
                "        self.history_len = history_len\n",
                "        \n",
                "    def forward(self, user, history, target_item):\n",
                "        # user: (batch_size)\n",
                "        # history: (batch_size, history_len)\n",
                "        # target_item: (batch_size)\n",
                "        \n",
                "        u_emb = self.user_embedding(user)  # (batch, dim)\n",
                "        \n",
                "        h_emb = self.item_embedding(history) # (batch, history_len, dim)\n",
                "        \n",
                "        # Attention Score\n",
                "        # We want to see which history items are relevant to the user's *current* preference (represented by u_emb)\n",
                "        # Score = u_emb . h_emb\n",
                "        \n",
                "        # Expand u_emb to match history dimension for dot product\n",
                "        u_emb_exp = u_emb.unsqueeze(1) # (batch, 1, dim)\n",
                "        \n",
                "        # Attention scores: (batch, history_len)\n",
                "        scores = torch.bmm(h_emb, u_emb_exp.transpose(1, 2)).squeeze(2) \n",
                "        \n",
                "        # Mask padding (0 is padding, but 0 is also a valid item ID? \n",
                "        # Assuming 0 is valid item, we should have used a special padding token or mask.\n",
                "        # For simplicity, let's assume item 0 is valid and just let attention learn.\n",
                "        # Or better, mask 0 if it's padding. Let's assume 0 is padding for now if we shifted IDs.\n",
                "        # But we didn't shift IDs. Let's ignore masking for simplicity or assume 0 is a valid item.\n",
                "        \n",
                "        attn_weights = torch.softmax(scores, dim=1) # (batch, history_len)\n",
                "        \n",
                "        # Weighted sum of history items\n",
                "        # (batch, 1, history_len) x (batch, history_len, dim) -> (batch, 1, dim)\n",
                "        user_history_vector = torch.bmm(attn_weights.unsqueeze(1), h_emb).squeeze(1)\n",
                "        \n",
                "        # Final User Representation: User Embedding + Weighted History\n",
                "        final_user_vector = u_emb + user_history_vector\n",
                "        \n",
                "        # Target Item Embedding\n",
                "        t_emb = self.item_embedding(target_item) # (batch, dim)\n",
                "        \n",
                "        # Prediction\n",
                "        prediction = (final_user_vector * t_emb).sum(dim=1)\n",
                "        \n",
                "        return torch.sigmoid(prediction)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = NPARecommender(n_users, n_items, EMBEDDING_DIM, HISTORY_LEN).to(device)\n",
                "criterion = nn.BCELoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 2571/2571 [00:27<00:00, 92.87it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/3, Loss: 5.1565\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 2571/2571 [00:27<00:00, 92.22it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/3, Loss: 4.0923\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 2571/2571 [00:28<00:00, 91.46it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3/3, Loss: 3.3441\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(\"Starting training...\")\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for user, history, item, label in tqdm(dataloader):\n",
                "        user, history, item, label = user.to(device), history.to(device), item.to(device), label.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        prediction = model(user, history, item)\n",
                "        loss = criterion(prediction, label)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        \n",
                "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(dataloader):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generate Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating recommendations...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 52643/52643 [00:16<00:00, 3181.31it/s]\n"
                    ]
                }
            ],
            "source": [
                "def generate_recommendations(model, user_history, n_users, n_items, output_file, top_k=20):\n",
                "    print(\"Generating recommendations...\")\n",
                "    model.eval()\n",
                "    \n",
                "    # Pre-compute item embeddings for fast retrieval\n",
                "    all_items = torch.arange(n_items).to(device)\n",
                "    item_embeddings = model.item_embedding(all_items) # (n_items, dim)\n",
                "    \n",
                "    with open(output_file, 'w') as f:\n",
                "        with torch.no_grad():\n",
                "            for user_id in tqdm(range(n_users)):\n",
                "                if user_id not in user_history:\n",
                "                    continue\n",
                "                    \n",
                "                # Prepare input\n",
                "                items = user_history[user_id]\n",
                "                if len(items) > HISTORY_LEN:\n",
                "                    hist_seq = items[-HISTORY_LEN:]\n",
                "                else:\n",
                "                    hist_seq = items + [0] * (HISTORY_LEN - len(items))\n",
                "                \n",
                "                user_tensor = torch.tensor([user_id]).to(device)\n",
                "                hist_tensor = torch.tensor([hist_seq]).to(device)\n",
                "                \n",
                "                # Compute User Vector\n",
                "                u_emb = model.user_embedding(user_tensor)\n",
                "                h_emb = model.item_embedding(hist_tensor)\n",
                "                \n",
                "                u_emb_exp = u_emb.unsqueeze(1)\n",
                "                scores = torch.bmm(h_emb, u_emb_exp.transpose(1, 2)).squeeze(2)\n",
                "                attn_weights = torch.softmax(scores, dim=1)\n",
                "                user_history_vector = torch.bmm(attn_weights.unsqueeze(1), h_emb).squeeze(1)\n",
                "                final_user_vector = u_emb + user_history_vector # (1, dim)\n",
                "                \n",
                "                # Compute scores for all items\n",
                "                # (1, dim) x (n_items, dim)^T -> (1, n_items)\n",
                "                item_scores = torch.matmul(final_user_vector, item_embeddings.T).squeeze(0)\n",
                "                \n",
                "                # Mask training items\n",
                "                item_scores[items] = -float('inf')\n",
                "                \n",
                "                # Top K\n",
                "                top_k_scores, top_k_indices = torch.topk(item_scores, top_k)\n",
                "                recs = top_k_indices.cpu().numpy()\n",
                "                \n",
                "                f.write(f\"{user_id} {' '.join(map(str, recs))}\\n\")\n",
                "                \n",
                "output_file = '/Users/riteshsingh/Documents/SJSU/Recommender System/projectrec/output_npa.txt'\n",
                "generate_recommendations(model, user_history, n_users, n_items, output_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
